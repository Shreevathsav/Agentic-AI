{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6888892",
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='gpt-oss:120b-cloud', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with question no explanation',\n",
    "  },\n",
    "])\n",
    "display(Markdown(response['message']['content']))\n",
    "question = response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-oss:120b-cloud'\n",
    "response = ollama.chat(model=model_name, messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content': question\n",
    "    }\n",
    "])\n",
    "answer = response['message']['content']\n",
    "display(Markdown(response['message']['content']))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-v3.1:671b-cloud'\n",
    "response = ollama.chat(model=model_name, messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content': question\n",
    "    }\n",
    "])\n",
    "answer = response['message']['content']\n",
    "display(Markdown(response['message']['content']))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'qwen3-coder:480b-cloud'\n",
    "response = ollama.chat(model=model_name, messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content': question\n",
    "    }\n",
    "])\n",
    "answer = response['message']['content']\n",
    "display(Markdown(response['message']['content']))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for competitor, answer in zip(competitors,answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9372b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = \"\"\n",
    "for index,answer in enumerate(answers):\n",
    "   together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "   together += answer + \"\\n\\n\"\n",
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359969a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competion between {len(competitors)} competitors.\n",
    "Each model has given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument and rank them inorder of best to worst.\n",
    "Respond with JSON and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\",...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "Now respond with JSON with ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cac0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_message =[{\"role\":\"user\", \"content\": judge}]\n",
    "print(judge_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-v3.1:671b-cloud'\n",
    "response = ollama.chat(model=model_name, messages=judge_message)\n",
    "results = response['message']['content']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index + 1}: {competitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multiple_llms)",
   "language": "python",
   "name": "multiple_llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
